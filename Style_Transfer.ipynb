{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Style_transfer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Giogia/gatys_piu_bello/blob/master/Style_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "xVS3ikr1mehX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "IMPORT"
      ]
    },
    {
      "metadata": {
        "id": "_gWU4wyx_6n_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "0bb57ef5-bd1c-406a-81cb-d2ab305d753e"
      },
      "cell_type": "code",
      "source": [
        "!pip install moviepy"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.6/dist-packages (0.2.3.5)\n",
            "Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.6/dist-packages (from moviepy) (2.4.1)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.6/dist-packages (from moviepy) (4.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from moviepy) (1.14.6)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.6/dist-packages (from moviepy) (4.28.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio<3.0,>=2.1.2->moviepy) (4.0.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->imageio<3.0,>=2.1.2->moviepy) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BmebnUUOBGRK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f1ac6a00-04dc-40dd-a907-f25de3a10c7d"
      },
      "cell_type": "code",
      "source": [
        "!imageio_download_bin ffmpeg"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ascertaining binaries for: ffmpeg.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "URLc5gsjl_Jt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.eager as tfe\n",
        "import tensorflow.keras as models\n",
        "\n",
        "from google.colab import files\n",
        "from tensorflow import clip_by_value\n",
        "from tensorflow import enable_eager_execution\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from numpy import clip, expand_dims, squeeze, array, random\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from pathlib import Path\n",
        "\n",
        "import moviepy.video.io.ffmpeg_writer as ffmpeg_writer\n",
        "import numpy as np\n",
        "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
        "from IPython.display import HTML, display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BWb96dmdgows",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "CNN"
      ]
    },
    {
      "metadata": {
        "id": "sVwQ_6NhgSRd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VGG19_c:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        # here you say where you want to take the features for the content\n",
        "        self.contentLayers = ['block4_conv2']\n",
        "\n",
        "        # here you say where you want to take the features for the style\n",
        "        self.styleLayers = ['block1_conv1',\n",
        "                              'block2_conv1',\n",
        "                              'block3_conv1',\n",
        "                              'block4_conv1',\n",
        "                              'block5_conv1']\n",
        "        self.content_layers_num = len(self.contentLayers)\n",
        "        self.style_layers_num = len(self.styleLayers)\n",
        "\n",
        "\n",
        "        self.model = self.getModel()\n",
        "\n",
        "        # after setting model not trainable we also set the layers not trainable\n",
        "        for layer in self.model.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "\n",
        "    def get_content_features(self,img):\n",
        "        return self.get_output_features(img)[0]\n",
        "\n",
        "\n",
        "    def get_style_features(self, img):\n",
        "        return self.get_output_features(img)[1]\n",
        "\n",
        "\n",
        "    def get_output_features(self, content):\n",
        "\n",
        "        features = self.model(content)\n",
        "\n",
        "        # for the content take only the content layers from 0 to len of content\n",
        "        content = [style_content[0] for style_content in features[self.style_layers_num:]]\n",
        "\n",
        "        # for style take only the style layers from len of content to len of content + len of style\n",
        "        style = [style[0] for style in features[:self.style_layers_num]]\n",
        "\n",
        "        return content, style\n",
        "\n",
        "\n",
        "    def getModel(self):\n",
        "\n",
        "        # we load the VGG19 pretrained with the dataset imagenet and we don't include the 3 fully connected layers on\n",
        "        # top of theVGG19\n",
        "        vgg = VGG19(include_top=False, weights='imagenet')\n",
        "\n",
        "        # we freeze the weights and the variables\n",
        "        vgg.trainable = False\n",
        "\n",
        "        style_feature = []\n",
        "        for i in self.styleLayers:\n",
        "            style_feature.append(vgg.get_layer(i).output)\n",
        "\n",
        "        content_feature = []\n",
        "        for i in self.contentLayers:\n",
        "            content_feature.append(vgg.get_layer(i).output)\n",
        "\n",
        "        #using the Keras API we return the model of the CNN\n",
        "        return models.Model(vgg.input, style_feature + content_feature)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bYTUfT2JgtV_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "IMAGE"
      ]
    },
    {
      "metadata": {
        "id": "p32FYPgpgdoO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_image(path):\n",
        "\n",
        "    max_dim = 1024\n",
        "\n",
        "    img = Image.open(path)\n",
        "\n",
        "    # resize image to max_dim\n",
        "    scale = max_dim / max(img.size)\n",
        "\n",
        "    if scale < 1:\n",
        "        scaled_width = round(img.size[0] * scale)\n",
        "        scaled_height = round(img.size[1] * scale)\n",
        "        img = img.resize((scaled_width, scaled_height))\n",
        "\n",
        "    #convert greyscale to rgb\n",
        "    img = img.convert(\"RGB\")\n",
        "\n",
        "    img = img_to_array(img)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def save_image(path, img):\n",
        "\n",
        "    img = Image.fromarray(clip(img, 0, 255).astype('uint8'))\n",
        "    img.save(path, 'JPEG')\n",
        "\n",
        "\n",
        "def preprocess_image(img):\n",
        "\n",
        "    img = expand_dims(img, axis=0)\n",
        "\n",
        "    #normalize by mean = [103.939, 116.779, 123.68] and with channels BGR\n",
        "    img = preprocess_input(img)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def postprocess_image(processed_img):\n",
        "\n",
        "    img = processed_img.copy()\n",
        "\n",
        "    # shape (1, h, w, d) to (h, w, d)\n",
        "    if len(img.shape) == 4:\n",
        "        img = squeeze(img, axis=0)\n",
        "    if len(img.shape) != 3:\n",
        "        raise ValueError(\"Invalid input to deprocessing image\")\n",
        "\n",
        "    # Remove VGG mean\n",
        "    img[:, :, 0] += 103.939\n",
        "    img[:, :, 1] += 116.779\n",
        "    img[:, :, 2] += 123.68\n",
        "\n",
        "    # rgb to bgr\n",
        "    img = img[:, :, ::-1]\n",
        "\n",
        "    #cast to values within (-255,255)\n",
        "    img = clip(img, 0, 255).astype('uint8')\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def clip_image(img):\n",
        "\n",
        "    norm_means = array([103.939, 116.779, 123.68])\n",
        "    min_vals = -norm_means\n",
        "    max_vals = 255 - norm_means\n",
        "\n",
        "    img = clip_by_value(img, min_vals, max_vals)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def show_image(img, title=None):\n",
        "\n",
        "    # Normalize for display\n",
        "    out = img.astype('uint8')\n",
        "\n",
        "    # Remove the batch dimension\n",
        "    if len(img.shape) == 4:\n",
        "        out = squeeze(out, axis=0)\n",
        "\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "\n",
        "    plt.imshow(out)\n",
        "\n",
        "\n",
        "def generate_noise_image(img):\n",
        "\n",
        "    img = random.uniform(-20,20,img.shape).astype('uint8')\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def show_content_style(content_path, style_path):\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "\n",
        "    content = load_image(content_path)\n",
        "    style = load_image(style_path)\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    show_image(content, 'Content')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    show_image(style, 'Style')\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cSkZidwqg4Uy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "LOSS"
      ]
    },
    {
      "metadata": {
        "id": "jKEPV9Mfg07z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def g_matrix(tensor):\n",
        "\n",
        "    channels = int(tensor.shape[-1])\n",
        "\n",
        "    # reshape as 1-Dim array dividing it per channel\n",
        "    a = tf.reshape(tensor, [-1, channels])\n",
        "\n",
        "    # compute the matrix a*a^t and then divide by the dimension\n",
        "    return tf.matmul(a, a, transpose_a=True) / tf.cast(tf.shape(a)[0], tf.float32)\n",
        "\n",
        "\n",
        "def get_content_loss(content, target):\n",
        "\n",
        "    return tf.reduce_mean(tf.square(content - target))\n",
        "\n",
        "\n",
        "def get_style_loss(style, g_target):\n",
        "\n",
        "    g_style = g_matrix(style)\n",
        "    height, width, channels = style.get_shape().as_list()\n",
        "    weight = channels ** 2\n",
        "\n",
        "\n",
        "    return tf.reduce_mean(tf.square(g_style - g_target))/weight\n",
        "\n",
        "\n",
        "def accumulate_loss(img_feature, layers_n, noise_feature, loss):\n",
        "\n",
        "    score = 0\n",
        "    weight_per_layer = 1.0 / float(layers_n)\n",
        "\n",
        "    for target, comb_content in zip(img_feature, noise_feature):\n",
        "        score += weight_per_layer * loss(comb_content, target)\n",
        "\n",
        "    return score\n",
        "\n",
        "\n",
        "def compute_loss(noise_features, img_features, loss_w, layers_n):\n",
        "    \"\"\"This function will compute the loss total loss.\n",
        "\n",
        "    Arguments:\n",
        "      noise_features: The content and style of the white noise\n",
        "      loss_w: The weights of each contribution of each loss function.\n",
        "        (style weight, content weight, and total variation weight)\n",
        "      img_features: Content and style features\n",
        "      loss_w: Weights of the elements\n",
        "      layers_n: Number of content and style layers\n",
        "\n",
        "    Returns:\n",
        "      returns the total loss\n",
        "    \"\"\"\n",
        "\n",
        "    # Accumulate content losses from all layers\n",
        "    content_score = accumulate_loss(img_features[0], layers_n[0], noise_features[0], get_content_loss)\n",
        "\n",
        "    # Accumulate style losses from all layers\n",
        "    style_score = accumulate_loss(img_features[1], layers_n[1], noise_features[1], get_style_loss)\n",
        "\n",
        "    # Here, we equally weight each contribution of each loss layer\n",
        "    content_score *= loss_w[0]\n",
        "    style_score *= loss_w[1]\n",
        "\n",
        "    return style_score + content_score\n",
        "\n",
        "\n",
        "def compute_gradient(noise_img, noise_features_gen, img_features, loss_w, layers_n):\n",
        "\n",
        "    with tf.GradientTape() as g:\n",
        "        loss = compute_loss(noise_features_gen(noise_img), img_features, loss_w, layers_n)\n",
        "\n",
        "    # Compute gradients wrt input image\n",
        "    return g.gradient(loss, noise_img), loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "80qyy1Aw62ys",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "IMAGE STYLE TRANSFER"
      ]
    },
    {
      "metadata": {
        "id": "IQwxfvI-gLkj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def image_style_transfer(content_path, style_path, output_path, iterations=1000, content_weight=1e0, style_weight=1e2, learning_rate=5):\n",
        "\n",
        "    #create images\n",
        "    content = load_image(content_path)\n",
        "    style = load_image(style_path)\n",
        "    noise = generate_noise_image(content)\n",
        "\n",
        "    content = preprocess_image(content)\n",
        "    style = preprocess_image(style)\n",
        "    noise = preprocess_image(noise)\n",
        "    percentage = 0\n",
        "    noise = percentage * noise + (1 - percentage) * content\n",
        "\n",
        "    noise = tfe.Variable(noise, dtype=tf.float32)\n",
        "\n",
        "    # create model\n",
        "    vgg = VGG19_c()\n",
        "    loss_weights = content_weight, style_weight\n",
        "    layers_number = vgg.content_layers_num , vgg.style_layers_num\n",
        "\n",
        "    #create features\n",
        "    content_features = vgg.get_content_features(content)\n",
        "    style_features = vgg.get_style_features(style)\n",
        "    gram_matrix_features = [g_matrix(feature) for feature in style_features]\n",
        "\n",
        "    img_features = content_features, gram_matrix_features\n",
        "\n",
        "    #create optimizer\n",
        "    opt = tf.train.AdamOptimizer(learning_rate, beta1=0.99, epsilon=1e-1)\n",
        "\n",
        "    #store best results\n",
        "    best_loss, best_img = float('inf'), None\n",
        "\n",
        "    #plt.ion()\n",
        "    for i in range(iterations):\n",
        "\n",
        "        grads, loss = compute_gradient(noise,vgg.get_output_features,img_features,loss_weights,layers_number)\n",
        "\n",
        "        opt.apply_gradients([(grads, noise)])\n",
        "\n",
        "        clipped = clip_image(noise)\n",
        "        noise.assign(clipped)\n",
        "\n",
        "\n",
        "        if loss < best_loss:\n",
        "\n",
        "            # Update best loss and best image from total loss.\n",
        "            best_loss = loss\n",
        "            best_img = postprocess_image(noise.numpy())\n",
        "\n",
        "        if i %100 == 0:\n",
        "            plot_img = noise.numpy()\n",
        "            plot_img = postprocess_image(plot_img)\n",
        "            show_image(plot_img)\n",
        "            plt.show()\n",
        "            print(\"Current Loss:  \" + str(loss.numpy()) + \"  Best Loss: \" + str(best_loss.numpy()))\n",
        "            out = display(progress(0, 100), display_id=True)\n",
        "        \n",
        "        out.update(progress(i, 100))\n",
        "\n",
        "    save_image(output_path,best_img)\n",
        "\n",
        "    return best_loss, best_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-fO62Mx0NYhS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "PROGRESS BAR"
      ]
    },
    {
      "metadata": {
        "id": "Ts_63i2dJBa9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nQztxZEO2AtJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TRANSFORM"
      ]
    },
    {
      "metadata": {
        "id": "5zDVCxH41vem",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "WEIGHTS_INIT_STDEV = .1\n",
        "\n",
        "\n",
        "def net(image):\n",
        "\n",
        "    conv1 = conv_layer(image, 32, 9, 1)\n",
        "    conv2 = conv_layer(conv1, 64, 3, 2)\n",
        "    conv3 = conv_layer(conv2, 128, 3, 2)\n",
        "    resid1 = residual_block(conv3, 3)\n",
        "    resid2 = residual_block(resid1, 3)\n",
        "    resid3 = residual_block(resid2, 3)\n",
        "    resid4 = residual_block(resid3, 3)\n",
        "    resid5 = residual_block(resid4, 3)\n",
        "    conv_t1 = conv_tranpose_layer(resid5, 64, 3, 2)\n",
        "    conv_t2 = conv_tranpose_layer(conv_t1, 32, 3, 2)\n",
        "    conv_t3 = conv_layer(conv_t2, 3, 9, 1, is_relu=False)\n",
        "    preds = tf.nn.tanh(conv_t3) * 150 + 255./2\n",
        "\n",
        "    return preds\n",
        "\n",
        "\n",
        "def conv_layer(image, filter_number, filter_size, strides, is_relu=True):\n",
        "\n",
        "    # make the convolution of the image and return the convolution\n",
        "\n",
        "    weights_initialization = conv_initialization_vars(image, filter_number, filter_size)\n",
        "    strides_shape = [1, strides, strides, 1]\n",
        "\n",
        "    # apply the filter to the image with a 2d convolution\n",
        "    image = tf.nn.conv2d(image, weights_initialization, strides_shape, padding='SAME')\n",
        "\n",
        "    image = _instance_norm(image)\n",
        "\n",
        "    if is_relu:\n",
        "\n",
        "        image = tf.nn.relu(image)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def conv_tranpose_layer(img, filter_number, filter_size, strides):\n",
        "\n",
        "    weights_initialized = conv_initialization_vars(img, filter_number, filter_size, transpose=True)\n",
        "\n",
        "    batch_size, rows, cols, in_channels = [i for i in img.get_shape()]\n",
        "    new_rows, new_cols = int(rows * strides), int(cols * strides)\n",
        "    new_shape = [batch_size, new_rows, new_cols, filter_number]\n",
        "\n",
        "    tf_shape = tf.stack(new_shape)\n",
        "    strides_shape = [1, strides, strides, 1]\n",
        "\n",
        "    convolution = tf.nn.conv2d_transpose(img, weights_initialized, tf_shape, strides_shape, padding='SAME')\n",
        "    convolution = _instance_norm(convolution)\n",
        "\n",
        "    return tf.nn.relu(convolution)\n",
        "\n",
        "\n",
        "def residual_block(img, filter_size=3):\n",
        "\n",
        "    tmp_convolution = conv_layer(img, 128, filter_size, 1)\n",
        "\n",
        "    # add the convolution to the original image\n",
        "    return img + conv_layer(tmp_convolution, 128, filter_size, 1, is_relu=False)\n",
        "\n",
        "\n",
        "def _instance_norm(img, train=True):\n",
        "\n",
        "    # set the shape of the input img\n",
        "    batch_size, rows, cols, in_channels = [i for i in img.get_shape()]\n",
        "\n",
        "    var_shape = [in_channels]\n",
        "\n",
        "    # calculate the mean and the variance of the img\n",
        "    mu, sigma_sq = tf.nn.moments(img, [1, 2], keep_dims=True)\n",
        "\n",
        "    shift = tf.Variable(tf.zeros(var_shape))\n",
        "    scale = tf.Variable(tf.ones(var_shape))\n",
        "\n",
        "    epsilon = 1e-3\n",
        "\n",
        "    # normalize the img input wrt the mean and the variance calculated\n",
        "    normalized = (img - mu) / (sigma_sq + epsilon) ** (.5)\n",
        "\n",
        "    return scale * normalized + shift\n",
        "\n",
        "\n",
        "def conv_initialization_vars(net, out_channels, filter_size, transpose=False):\n",
        "\n",
        "    _, rows, cols, in_channels = [i.value for i in net.get_shape()]\n",
        "\n",
        "    if not transpose:\n",
        "\n",
        "        weights_shape = [filter_size, filter_size, in_channels, out_channels]\n",
        "\n",
        "    else:\n",
        "\n",
        "        weights_shape = [filter_size, filter_size, out_channels, in_channels]\n",
        "\n",
        "    # with tf truncated we output rnd values rom a truncated normal distribution\n",
        "    weights_init = tf.Variable(tf.truncated_normal(weights_shape, stddev=WEIGHTS_INIT_STDEV, seed=1), dtype=tf.float32)\n",
        "\n",
        "    return weights_init"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BKscEyJX2D4_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "VIDEO STYLE TRANSFER"
      ]
    },
    {
      "metadata": {
        "id": "woxCbccY2MD_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DEVICE = '/gpu:0'\n",
        "EX = Exception(\"Please provide a model\")\n",
        "\n",
        "\n",
        "def video_style_transfer(input_path, model_path, output_path, batch_s=4):\n",
        "\n",
        "    video = VideoFileClip(input_path, audio=False)\n",
        "    video_w = ffmpeg_writer.FFMPEG_VideoWriter(output_path, video.size, video.fps, codec=\"libx264\",\n",
        "                                               preset=\"medium\", bitrate=\"2000k\",\n",
        "                                               audiofile=input_path, threads=None,\n",
        "                                               ffmpeg_params=None)\n",
        "\n",
        "    with tf.Graph().as_default(), tf.Session() as session:\n",
        "\n",
        "        video_iter = list(video.iter_frames())\n",
        "        batch_l = [video_iter[i:i + batch_s] for i in range(0, len(video_iter), batch_s)]\n",
        "        while len(batch_l[-1]) < batch_s:\n",
        "            batch_l[-1].append(batch_l[-1][-1])\n",
        "\n",
        "        video_wip = np.array(batch_l, dtype=np.float32)\n",
        "        place_holder = tf.placeholder(tf.float32, shape=video_wip.shape[1:], name='place_holder')\n",
        "        wip = net(place_holder)\n",
        "\n",
        "        p_loader = tf.train.Saver()\n",
        "\n",
        "        if os.path.isdir(model_path):\n",
        "\n",
        "            model = tf.train.get_checkpoint_state(model_path)\n",
        "            is_valid = model.model_checkpoint_path\n",
        "\n",
        "            if model is not None and is_valid:\n",
        "                print(\"Loading model, it may take some time\")\n",
        "                p_loader.restore(session, is_valid)\n",
        "            else:\n",
        "                raise EX\n",
        "        else:\n",
        "            p_loader.restore(session, model_path)\n",
        "\n",
        "        # The information about size in the video files are: 'width, height'\n",
        "        # In *** the dimensions are 'height, width'\n",
        "        #shape = (batch_s, video.size[1], video.size[0], 3)\n",
        "        # TODO check if it's ok without shape\n",
        "        for i in range(len(video_wip)):\n",
        "            r_res = session.run(wip, feed_dict={place_holder: video_wip[i]})\n",
        "            for r in r_res:\n",
        "                video_w.write_frame(np.clip(r, 0, 255).astype(np.uint8))\n",
        "            print(\"processed \" + str(i+1) + \" out of \" + str(len(video_wip)) + \" batches\", end = '\\r')\n",
        "\n",
        "        video_w.close()\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K2vd75pWgxOB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "STYLE TRANSFER"
      ]
    },
    {
      "metadata": {
        "id": "bRiZZYOFiJmM",
        "colab_type": "code",
        "outputId": "5d4f5cac-da1f-452a-fa4e-1eba4213ad04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "cell_type": "code",
      "source": [
        "PATH = os.getcwd()\n",
        "IMAGES_PATH = PATH + '/Images'\n",
        "VIDEOS_PATH = PATH + '/Videos'\n",
        "MODELS_PATH = PATH + '/Models'\n",
        "RESULTS_PATH = PATH + '/Results'\n",
        "\n",
        "\n",
        "def find_file(filename, directory):\n",
        "    for file in os.listdir(directory):\n",
        "        if os.path.splitext(file)[0] == filename:\n",
        "            return file\n",
        "\n",
        "    raise Exception(\"File not found\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    #tf.enable_eager_execution()\n",
        "    print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
        "        \n",
        "    choice = input(\"Select one of the following options:\\n\"\n",
        "                    \"1 - Style Transfer for Images\\n\"\n",
        "                    \"2 - Style Transfer for Videos\\n\")\n",
        "\n",
        "    \n",
        "    if choice == '1':\n",
        "      \n",
        "        if not(os.path.isdir(IMAGES_PATH)):\n",
        "            os.mkdir(IMAGES_PATH)\n",
        "        while(len(os.listdir(IMAGES_PATH))<2):\n",
        "            print(\"Please upload at least two image\")\n",
        "            files.upload()\n",
        "\n",
        "        print(\"Select Content Image:\")\n",
        "\n",
        "        for file in os.listdir(Path(IMAGES_PATH)):\n",
        "            print(os.path.splitext(file)[0])\n",
        "\n",
        "        content = find_file(input(),Path(IMAGES_PATH))\n",
        "        content_path  = Path(IMAGES_PATH + \"/\" + content)\n",
        "\n",
        "        print(\"Select Style Image:\")\n",
        "\n",
        "        for file in os.listdir(Path(IMAGES_PATH)):\n",
        "            print(os.path.splitext(file)[0])\n",
        "\n",
        "        style = find_file(input(),Path(IMAGES_PATH))\n",
        "        style_path = Path(IMAGES_PATH + \"/\" + style)\n",
        "        \n",
        "        if not(os.path.isdir(RESULTS_PATH)):\n",
        "            os.mkdir(RESULTS_PATH)\n",
        "        output =  'Result' + '_' + os.path.splitext(content[0]) + '_' + os.path.splitext(style[0])\n",
        "        output_path = Path(RESULTS_PATH + \"/\" + output + \".jpg\")\n",
        "\n",
        "\n",
        "        show_content_style(content_path, style_path)\n",
        "\n",
        "        #print(\"Please wait, ignore tensorflow binary value warning\")\n",
        "        best_img, best_loss = image_style_transfer(content_path, style_path, output_path, iterations=5000)\n",
        "\n",
        "        print(\"Final Loss: \" + best_loss.numpy)\n",
        "\n",
        "        show_image(best_img, output)\n",
        "\n",
        "        print(\"image saved in Results folder\")\n",
        "        \n",
        "        files.download(output+'.jpg')\n",
        "\n",
        "\n",
        "    if choice == '2':\n",
        "      \n",
        "        os.mkdir(VIDEOS_PATH)\n",
        "        while(len(os.listdir(VIDEOS_PATH))<1):\n",
        "            print(\"Please upload at least one video\")\n",
        "            files.upload()\n",
        "    \n",
        "        os.mkdir(MODELS_PATH)\n",
        "        while(len(os.listdir(MODELS_PATH))<1):\n",
        "            print(\"Please upload at least one model\")\n",
        "            files.upload()\n",
        "\n",
        "        print(\"Select Content Video:\")\n",
        "\n",
        "        for file in os.listdir(Path(VIDEOS_PATH)):\n",
        "            print(os.path.splitext(file)[0])\n",
        "\n",
        "        content = find_file(input(),Path(VIDEOS_PATH))\n",
        "        content_path  = Path(VIDEOS_PATH + \"/\" + content)\n",
        "\n",
        "        print(\"Select Style Model:\")\n",
        "\n",
        "        for file in os.listdir(Path(MODELS_PATH)):\n",
        "            print(os.path.splitext(file)[0])\n",
        "\n",
        "        model = find_file(input(),Path(MODELS_PATH))\n",
        "        model_path = Path(MODELS_PATH + \"/\" + model)\n",
        "\n",
        "        os.mkdir(RESULTS_PATH)\n",
        "        output =  'Result' + '_' + os.path.splitext(content)[0] + '_' + os.path.splitext(model)[0]\n",
        "        output_path = Path(RESULTS_PATH + \"/\" + output + \".mp4\")\n",
        "\n",
        "        print(\"Please wait, ignore tensorflow binary value warning\")\n",
        "        video_style_transfer(str(content_path), str(model_path), str(output_path), batch_s=4)\n",
        "\n",
        "        print(\"video saved in Files folder, refresh to see it\")\n",
        "        \n",
        "        files.download(output+'.mp4')"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Eager execution: False\n",
            "Select one of the following options:\n",
            "1 - Style Transfer for Images\n",
            "2 - Style Transfer for Videos\n",
            "1\n",
            "Please upload at least two image\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-138-482a441d4461>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMAGES_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please upload at least two image\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Select Content Image:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Images' is not defined"
          ]
        }
      ]
    }
  ]
}