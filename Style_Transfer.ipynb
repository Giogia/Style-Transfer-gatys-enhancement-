{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Style_transfer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Giogia/gatys_piu_bello/blob/master/Style_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "xVS3ikr1mehX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "IMPORT"
      ]
    },
    {
      "metadata": {
        "id": "_gWU4wyx_6n_",
        "colab_type": "code",
        "outputId": "badc6c33-c98a-406a-8bd5-2f6a18c01b3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install moviepy"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.6/dist-packages (0.2.3.5)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.6/dist-packages (from moviepy) (4.3.0)\n",
            "Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.6/dist-packages (from moviepy) (2.4.1)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.6/dist-packages (from moviepy) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from moviepy) (1.14.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio<3.0,>=2.1.2->moviepy) (4.0.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->imageio<3.0,>=2.1.2->moviepy) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BmebnUUOBGRK",
        "colab_type": "code",
        "outputId": "4f21d493-8f72-4315-891c-46762e2ff4d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!imageio_download_bin ffmpeg"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ascertaining binaries for: ffmpeg.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "URLc5gsjl_Jt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.eager as tfe\n",
        "import tensorflow.keras as models\n",
        "\n",
        "from google.colab import files\n",
        "from tensorflow import clip_by_value\n",
        "from tensorflow import enable_eager_execution\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from numpy import clip, expand_dims, squeeze, array, random\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from pathlib import Path\n",
        "\n",
        "import moviepy.video.io.ffmpeg_writer as ffmpeg_writer\n",
        "import numpy as np\n",
        "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
        "from IPython.display import HTML, display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BWb96dmdgows",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "CNN"
      ]
    },
    {
      "metadata": {
        "id": "sVwQ_6NhgSRd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VGG19_c:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        # here you say where you want to take the features for the content\n",
        "        self.contentLayers = ['block4_conv2']\n",
        "\n",
        "        # here you say where you want to take the features for the style\n",
        "        self.styleLayers = ['block1_conv1',\n",
        "                              'block2_conv1',\n",
        "                              'block3_conv1',\n",
        "                              'block4_conv1',\n",
        "                              'block5_conv1']\n",
        "        self.content_layers_num = len(self.contentLayers)\n",
        "        self.style_layers_num = len(self.styleLayers)\n",
        "\n",
        "\n",
        "        self.model = self.getModel()\n",
        "\n",
        "        # after setting model not trainable we also set the layers not trainable\n",
        "        for layer in self.model.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "\n",
        "    def get_content_features(self,img):\n",
        "        return self.get_output_features(img)[0]\n",
        "\n",
        "\n",
        "    def get_style_features(self, img):\n",
        "        return self.get_output_features(img)[1]\n",
        "\n",
        "\n",
        "    def get_output_features(self, content):\n",
        "\n",
        "        features = self.model(content)\n",
        "\n",
        "        # for the content take only the content layers from 0 to len of content\n",
        "        content = [style_content[0] for style_content in features[self.style_layers_num:]]\n",
        "\n",
        "        # for style take only the style layers from len of content to len of content + len of style\n",
        "        style = [style[0] for style in features[:self.style_layers_num]]\n",
        "\n",
        "        return content, style\n",
        "\n",
        "\n",
        "    def getModel(self):\n",
        "\n",
        "        # we load the VGG19 pretrained with the dataset imagenet and we don't include the 3 fully connected layers on\n",
        "        # top of theVGG19\n",
        "        vgg = VGG19(include_top=False, weights='imagenet')\n",
        "\n",
        "        # we freeze the weights and the variables\n",
        "        vgg.trainable = False\n",
        "\n",
        "        style_feature = []\n",
        "        for i in self.styleLayers:\n",
        "            style_feature.append(vgg.get_layer(i).output)\n",
        "\n",
        "        content_feature = []\n",
        "        for i in self.contentLayers:\n",
        "            content_feature.append(vgg.get_layer(i).output)\n",
        "\n",
        "        #using the Keras API we return the model of the CNN\n",
        "        return models.Model(vgg.input, style_feature + content_feature)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bYTUfT2JgtV_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "IMAGE"
      ]
    },
    {
      "metadata": {
        "id": "p32FYPgpgdoO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_image(path):\n",
        "\n",
        "    max_dim = 1024\n",
        "\n",
        "    img = Image.open(path)\n",
        "\n",
        "    # resize image to max_dim\n",
        "    scale = max_dim / max(img.size)\n",
        "\n",
        "    if scale < 1:\n",
        "        scaled_width = round(img.size[0] * scale)\n",
        "        scaled_height = round(img.size[1] * scale)\n",
        "        img = img.resize((scaled_width, scaled_height))\n",
        "\n",
        "    #convert greyscale to rgb\n",
        "    img = img.convert(\"RGB\")\n",
        "\n",
        "    img = img_to_array(img)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def save_image(path, img):\n",
        "\n",
        "    img = Image.fromarray(clip(img, 0, 255).astype('uint8'))\n",
        "    img.save(path, 'JPEG')\n",
        "\n",
        "\n",
        "def preprocess_image(img):\n",
        "\n",
        "    img = expand_dims(img, axis=0)\n",
        "\n",
        "    #normalize by mean = [103.939, 116.779, 123.68] and with channels BGR\n",
        "    img = preprocess_input(img)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def postprocess_image(processed_img):\n",
        "\n",
        "    img = processed_img.copy()\n",
        "\n",
        "    # shape (1, h, w, d) to (h, w, d)\n",
        "    if len(img.shape) == 4:\n",
        "        img = squeeze(img, axis=0)\n",
        "    if len(img.shape) != 3:\n",
        "        raise ValueError(\"Invalid input to deprocessing image\")\n",
        "\n",
        "    # Remove VGG mean\n",
        "    img[:, :, 0] += 103.939\n",
        "    img[:, :, 1] += 116.779\n",
        "    img[:, :, 2] += 123.68\n",
        "\n",
        "    # rgb to bgr\n",
        "    img = img[:, :, ::-1]\n",
        "\n",
        "    #cast to values within (-255,255)\n",
        "    img = clip(img, 0, 255).astype('uint8')\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def clip_image(img):\n",
        "\n",
        "    norm_means = array([103.939, 116.779, 123.68])\n",
        "    min_vals = -norm_means\n",
        "    max_vals = 255 - norm_means\n",
        "\n",
        "    img = clip_by_value(img, min_vals, max_vals)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def show_image(img, title=None):\n",
        "\n",
        "    # Normalize for display\n",
        "    out = img.astype('uint8')\n",
        "\n",
        "    # Remove the batch dimension\n",
        "    if len(img.shape) == 4:\n",
        "        out = squeeze(out, axis=0)\n",
        "\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "\n",
        "    plt.imshow(out)\n",
        "\n",
        "\n",
        "def generate_noise_image(img):\n",
        "\n",
        "    img = random.uniform(-20,20,img.shape).astype('uint8')\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def show_content_style(content_path, style_path):\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "\n",
        "    content = load_image(content_path)\n",
        "    style = load_image(style_path)\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    show_image(content, 'Content')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    show_image(style, 'Style')\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cSkZidwqg4Uy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "LOSS"
      ]
    },
    {
      "metadata": {
        "id": "jKEPV9Mfg07z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def g_matrix(tensor):\n",
        "\n",
        "    channels = int(tensor.shape[-1])\n",
        "\n",
        "    # reshape as 1-Dim array dividing it per channel\n",
        "    a = tf.reshape(tensor, [-1, channels])\n",
        "\n",
        "    # compute the matrix a*a^t and then divide by the dimension\n",
        "    return tf.matmul(a, a, transpose_a=True) / tf.cast(tf.shape(a)[0], tf.float32)\n",
        "\n",
        "\n",
        "def get_content_loss(content, target):\n",
        "\n",
        "    return tf.reduce_mean(tf.square(content - target))\n",
        "\n",
        "\n",
        "def get_style_loss(style, g_target):\n",
        "\n",
        "    g_style = g_matrix(style)\n",
        "    height, width, channels = style.get_shape().as_list()\n",
        "    weight = channels ** 2\n",
        "\n",
        "\n",
        "    return tf.reduce_mean(tf.square(g_style - g_target))/weight\n",
        "\n",
        "\n",
        "def accumulate_loss(img_feature, layers_n, noise_feature, loss):\n",
        "\n",
        "    score = 0\n",
        "    weight_per_layer = 1.0 / float(layers_n)\n",
        "\n",
        "    for target, comb_content in zip(img_feature, noise_feature):\n",
        "        score += weight_per_layer * loss(comb_content, target)\n",
        "\n",
        "    return score\n",
        "\n",
        "\n",
        "def compute_loss(noise_features, img_features, loss_w, layers_n):\n",
        "    \"\"\"This function will compute the loss total loss.\n",
        "\n",
        "    Arguments:\n",
        "      noise_features: The content and style of the white noise\n",
        "      loss_w: The weights of each contribution of each loss function.\n",
        "        (style weight, content weight, and total variation weight)\n",
        "      img_features: Content and style features\n",
        "      loss_w: Weights of the elements\n",
        "      layers_n: Number of content and style layers\n",
        "\n",
        "    Returns:\n",
        "      returns the total loss\n",
        "    \"\"\"\n",
        "\n",
        "    # Accumulate content losses from all layers\n",
        "    content_score = accumulate_loss(img_features[0], layers_n[0], noise_features[0], get_content_loss)\n",
        "\n",
        "    # Accumulate style losses from all layers\n",
        "    style_score = accumulate_loss(img_features[1], layers_n[1], noise_features[1], get_style_loss)\n",
        "\n",
        "    # Here, we equally weight each contribution of each loss layer\n",
        "    content_score *= loss_w[0]\n",
        "    style_score *= loss_w[1]\n",
        "\n",
        "    return style_score + content_score\n",
        "\n",
        "\n",
        "def compute_gradient(noise_img, noise_features_gen, img_features, loss_w, layers_n):\n",
        "\n",
        "    with tf.GradientTape() as g:\n",
        "        loss = compute_loss(noise_features_gen(noise_img), img_features, loss_w, layers_n)\n",
        "\n",
        "    # Compute gradients wrt input image\n",
        "    return g.gradient(loss, noise_img), loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "80qyy1Aw62ys",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "IMAGE STYLE TRANSFER"
      ]
    },
    {
      "metadata": {
        "id": "IQwxfvI-gLkj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def image_style_transfer(content_path, style_path, output_path, iterations=1000, content_weight=1e0, style_weight=1e2, learning_rate=5):\n",
        "\n",
        "    #create images\n",
        "    content = load_image(content_path)\n",
        "    style = load_image(style_path)\n",
        "    noise = generate_noise_image(content)\n",
        "\n",
        "    content = preprocess_image(content)\n",
        "    style = preprocess_image(style)\n",
        "    noise = preprocess_image(noise)\n",
        "    percentage = 0\n",
        "    noise = percentage * noise + (1 - percentage) * content\n",
        "\n",
        "    noise = tfe.Variable(noise, dtype=tf.float32)\n",
        "\n",
        "    # create model\n",
        "    vgg = VGG19_c()\n",
        "    loss_weights = content_weight, style_weight\n",
        "    layers_number = vgg.content_layers_num , vgg.style_layers_num\n",
        "\n",
        "    #create features\n",
        "    content_features = vgg.get_content_features(content)\n",
        "    style_features = vgg.get_style_features(style)\n",
        "    gram_matrix_features = [g_matrix(feature) for feature in style_features]\n",
        "\n",
        "    img_features = content_features, gram_matrix_features\n",
        "\n",
        "    #create optimizer\n",
        "    opt = tf.train.AdamOptimizer(learning_rate, beta1=0.99, epsilon=1e-1)\n",
        "\n",
        "    #store best results\n",
        "    best_loss, best_img = float('inf'), None\n",
        "\n",
        "    #plt.ion()\n",
        "    for i in range(iterations):\n",
        "\n",
        "        grads, loss = compute_gradient(noise,vgg.get_output_features,img_features,loss_weights,layers_number)\n",
        "\n",
        "        opt.apply_gradients([(grads, noise)])\n",
        "\n",
        "        clipped = clip_image(noise)\n",
        "        noise.assign(clipped)\n",
        "\n",
        "\n",
        "        if loss < best_loss:\n",
        "\n",
        "            # Update best loss and best image from total loss.\n",
        "            best_loss = loss\n",
        "            best_img = postprocess_image(noise.numpy())\n",
        "\n",
        "        if i %100 == 0:\n",
        "            print(\"Iterations:\" + str(i))\n",
        "            plot_img = noise.numpy()\n",
        "            plot_img = postprocess_image(plot_img)\n",
        "            show_image(plot_img)\n",
        "            plt.show()\n",
        "            print(\"Current Loss:\" +str(loss.numpy())+\"  Best Loss:\"+str(best_loss.numpy()))\n",
        "            progressbar = display(progress(0,99), display_id=True)\n",
        "            \n",
        "        progressbar.update(progress(i%100, 99))\n",
        "\n",
        "    save_image(output_path,best_img)\n",
        "\n",
        "    return best_loss, best_img\n",
        "  \n",
        " \n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-fO62Mx0NYhS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "PROGRESS BAR"
      ]
    },
    {
      "metadata": {
        "id": "Ts_63i2dJBa9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nQztxZEO2AtJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TRANSFORM"
      ]
    },
    {
      "metadata": {
        "id": "5zDVCxH41vem",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "WEIGHTS_INIT_STDEV = .1\n",
        "\n",
        "\n",
        "def net(image):\n",
        "\n",
        "    conv1 = conv_layer(image, 32, 9, 1)\n",
        "    conv2 = conv_layer(conv1, 64, 3, 2)\n",
        "    conv3 = conv_layer(conv2, 128, 3, 2)\n",
        "    resid1 = residual_block(conv3, 3)\n",
        "    resid2 = residual_block(resid1, 3)\n",
        "    resid3 = residual_block(resid2, 3)\n",
        "    resid4 = residual_block(resid3, 3)\n",
        "    resid5 = residual_block(resid4, 3)\n",
        "    conv_t1 = conv_tranpose_layer(resid5, 64, 3, 2)\n",
        "    conv_t2 = conv_tranpose_layer(conv_t1, 32, 3, 2)\n",
        "    conv_t3 = conv_layer(conv_t2, 3, 9, 1, is_relu=False)\n",
        "    preds = tf.nn.tanh(conv_t3) * 150 + 255./2\n",
        "\n",
        "    return preds\n",
        "\n",
        "\n",
        "def conv_layer(image, filter_number, filter_size, strides, is_relu=True):\n",
        "\n",
        "    # make the convolution of the image and return the convolution\n",
        "\n",
        "    weights_initialization = conv_initialization_vars(image, filter_number, filter_size)\n",
        "    strides_shape = [1, strides, strides, 1]\n",
        "\n",
        "    # apply the filter to the image with a 2d convolution\n",
        "    image = tf.nn.conv2d(image, weights_initialization, strides_shape, padding='SAME')\n",
        "\n",
        "    image = _instance_norm(image)\n",
        "\n",
        "    if is_relu:\n",
        "\n",
        "        image = tf.nn.relu(image)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def conv_tranpose_layer(img, filter_number, filter_size, strides):\n",
        "\n",
        "    weights_initialized = conv_initialization_vars(img, filter_number, filter_size, transpose=True)\n",
        "\n",
        "    batch_size, rows, cols, in_channels = [i for i in img.get_shape()]\n",
        "    new_rows, new_cols = int(rows * strides), int(cols * strides)\n",
        "    new_shape = [batch_size, new_rows, new_cols, filter_number]\n",
        "\n",
        "    tf_shape = tf.stack(new_shape)\n",
        "    strides_shape = [1, strides, strides, 1]\n",
        "\n",
        "    convolution = tf.nn.conv2d_transpose(img, weights_initialized, tf_shape, strides_shape, padding='SAME')\n",
        "    convolution = _instance_norm(convolution)\n",
        "\n",
        "    return tf.nn.relu(convolution)\n",
        "\n",
        "\n",
        "def residual_block(img, filter_size=3):\n",
        "\n",
        "    tmp_convolution = conv_layer(img, 128, filter_size, 1)\n",
        "\n",
        "    # add the convolution to the original image\n",
        "    return img + conv_layer(tmp_convolution, 128, filter_size, 1, is_relu=False)\n",
        "\n",
        "\n",
        "def _instance_norm(img, train=True):\n",
        "\n",
        "    # set the shape of the input img\n",
        "    batch_size, rows, cols, in_channels = [i for i in img.get_shape()]\n",
        "\n",
        "    var_shape = [in_channels]\n",
        "\n",
        "    # calculate the mean and the variance of the img\n",
        "    mu, sigma_sq = tf.nn.moments(img, [1, 2], keep_dims=True)\n",
        "\n",
        "    shift = tf.Variable(tf.zeros(var_shape))\n",
        "    scale = tf.Variable(tf.ones(var_shape))\n",
        "\n",
        "    epsilon = 1e-3\n",
        "\n",
        "    # normalize the img input wrt the mean and the variance calculated\n",
        "    normalized = (img - mu) / (sigma_sq + epsilon) ** (.5)\n",
        "\n",
        "    return scale * normalized + shift\n",
        "\n",
        "\n",
        "def conv_initialization_vars(net, out_channels, filter_size, transpose=False):\n",
        "\n",
        "    _, rows, cols, in_channels = [i.value for i in net.get_shape()]\n",
        "\n",
        "    if not transpose:\n",
        "\n",
        "        weights_shape = [filter_size, filter_size, in_channels, out_channels]\n",
        "\n",
        "    else:\n",
        "\n",
        "        weights_shape = [filter_size, filter_size, out_channels, in_channels]\n",
        "\n",
        "    # with tf truncated we output rnd values rom a truncated normal distribution\n",
        "    weights_init = tf.Variable(tf.truncated_normal(weights_shape, stddev=WEIGHTS_INIT_STDEV, seed=1), dtype=tf.float32)\n",
        "\n",
        "    return weights_init"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BKscEyJX2D4_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "VIDEO STYLE TRANSFER"
      ]
    },
    {
      "metadata": {
        "id": "woxCbccY2MD_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DEVICE = '/gpu:0'\n",
        "EX = Exception(\"Please provide a model\")\n",
        "\n",
        "\n",
        "def video_style_transfer(input_path, model_path, output_path, batch_s=4):\n",
        "\n",
        "    video = VideoFileClip(input_path, audio=False)\n",
        "    video_w = ffmpeg_writer.FFMPEG_VideoWriter(output_path, video.size, video.fps, codec=\"libx264\",\n",
        "                                               preset=\"medium\", bitrate=\"2000k\",\n",
        "                                               audiofile=input_path, threads=None,\n",
        "                                               ffmpeg_params=None)\n",
        "\n",
        "    with tf.Graph().as_default(), tf.Session() as session:\n",
        "\n",
        "        video_iter = list(video.iter_frames())\n",
        "        batch_l = [video_iter[i:i + batch_s] for i in range(0, len(video_iter), batch_s)]\n",
        "        while len(batch_l[-1]) < batch_s:\n",
        "            batch_l[-1].append(batch_l[-1][-1])\n",
        "\n",
        "        video_wip = np.array(batch_l, dtype=np.float32)\n",
        "        place_holder = tf.placeholder(tf.float32, shape=video_wip.shape[1:], name='place_holder')\n",
        "        wip = net(place_holder)\n",
        "\n",
        "        p_loader = tf.train.Saver()\n",
        "        print(\"Loading model, it may take some time\")\n",
        "                \n",
        "        if os.path.isdir(model_path):\n",
        "\n",
        "            model = tf.train.get_checkpoint_state(model_path)\n",
        "            is_valid = model.model_checkpoint_path\n",
        "\n",
        "            if model is not None and is_valid:\n",
        "                p_loader.restore(session, is_valid)\n",
        "            else:\n",
        "                raise EX\n",
        "        else:\n",
        "            p_loader.restore(session, model_path)\n",
        "\n",
        "        # The information about size in the video files are: 'width, height'\n",
        "        # In *** the dimensions are 'height, width'\n",
        "        #shape = (batch_s, video.size[1], video.size[0], 3)\n",
        "        # TODO check if it's ok without shape\n",
        "        progressbar = display(progress(0,len(video_wip)-1), display_id=True)\n",
        "        for i in range(len(video_wip)):\n",
        "          \n",
        "            progressbar.update(progress(i,len(video_wip)-1))\n",
        "            r_res = session.run(wip, feed_dict={place_holder: video_wip[i]})\n",
        "            for r in r_res:\n",
        "                video_w.write_frame(np.clip(r, 0, 255).astype(np.uint8))\n",
        "            #print(\"processed \" + str(i+1) + \" out of \" + str(len(video_wip)) + \" batches\", end = '\\r')\n",
        "\n",
        "        video_w.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K2vd75pWgxOB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "STYLE TRANSFER"
      ]
    },
    {
      "metadata": {
        "id": "bRiZZYOFiJmM",
        "colab_type": "code",
        "outputId": "7ae9d2e6-9e85-41a5-fb1e-57c8ed811ba2",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "cell_type": "code",
      "source": [
        "PATH = os.getcwd()\n",
        "IMAGES_PATH = PATH + '/Images'\n",
        "VIDEOS_PATH = PATH + '/Videos'\n",
        "MODELS_PATH = PATH + '/Models'\n",
        "RESULTS_PATH = PATH + '/Results'\n",
        "\n",
        "\n",
        "def find_file(filename, directory):\n",
        "    for file in os.listdir(directory):\n",
        "        if os.path.splitext(file)[0] == filename:\n",
        "            return file\n",
        "\n",
        "    raise Exception(\"File not found\")\n",
        "\n",
        "    \n",
        "def create_directory(path):\n",
        "    \n",
        "    if not(os.path.isdir(path)):\n",
        "            os.mkdir(path)\n",
        "  \n",
        "  \n",
        "def upload_files(path,number, message):\n",
        "    \n",
        "    while(len(os.listdir(path))<number):\n",
        "            print(\"Please upload at least \"+message)\n",
        "            os.chdir(path)\n",
        "            files.upload()\n",
        "            os.chdir(PATH)\n",
        "    \n",
        "      \n",
        "def list_files(path):\n",
        "  \n",
        "    for file in os.listdir(Path(path)):\n",
        "            print(os.path.splitext(file)[0])\n",
        "        \n",
        "        \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    tf.enable_eager_execution()\n",
        "    #print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
        "        \n",
        "    choice = input(\"Select one of the following options:\\n\"\n",
        "                    \"1 - Style Transfer for Images\\n\"\n",
        "                    \"2 - Style Transfer for Videos\\n\")\n",
        "\n",
        "    \n",
        "    if choice == '1':\n",
        "\n",
        "        create_directory(IMAGES_PATH)\n",
        "        upload_files(IMAGES_PATH,2,\"two images\")\n",
        "\n",
        "        print(\"Select Content Image:\")\n",
        "\n",
        "        list_files(IMAGES_PATH)\n",
        "\n",
        "        content = find_file(input(),Path(IMAGES_PATH))\n",
        "        content_path  = Path(IMAGES_PATH + \"/\" + content)\n",
        "\n",
        "        print(\"Select Style Image:\")\n",
        "\n",
        "        list_files(IMAGES_PATH)\n",
        "\n",
        "        style = find_file(input(),Path(IMAGES_PATH))\n",
        "        style_path = Path(IMAGES_PATH + \"/\" + style)\n",
        "        \n",
        "        create_directory(RESULTS_PATH)\n",
        "        \n",
        "        output =  'Result' + '_' + os.path.splitext(content)[0] + '_' + os.path.splitext(style)[0]\n",
        "        output_path = Path(RESULTS_PATH + \"/\" + output + \".jpg\")\n",
        "\n",
        "        show_content_style(content_path, style_path)\n",
        "\n",
        "        best_loss,best_img = image_style_transfer(content_path, style_path, output_path, iterations=5000)\n",
        "\n",
        "        print(\"Final Loss: \" + str(best_loss.numpy()))\n",
        "        show_image(best_img, output)\n",
        "        plt.show()\n",
        "\n",
        "        print(\"image saved in Results folder, refresh Files to see it\")\n",
        "        \n",
        "        files.download(output_path)\n",
        "        \n",
        "\n",
        "    if choice == '2':\n",
        "      \n",
        "        create_directory(VIDEOS_PATH)\n",
        "        upload_files(VIDEOS_PATH,1,\"one video\")\n",
        "   \n",
        "        print(\"Select Content Video:\")\n",
        "\n",
        "        list_files(VIDEOS_PATH)\n",
        "        \n",
        "        content = find_file(input(),Path(VIDEOS_PATH))\n",
        "        content_path  = Path(VIDEOS_PATH + \"/\" + content)\n",
        "\n",
        "        create_directory(MODELS_PATH)\n",
        "        upload_files(MODELS_PATH,1,\"one model\")\n",
        "\n",
        "        print(\"Select Style Model:\")\n",
        "\n",
        "        list_files(MODELS_PATH)\n",
        "\n",
        "        model = find_file(input(),Path(MODELS_PATH))\n",
        "        model_path = Path(MODELS_PATH + \"/\" + model)\n",
        "\n",
        "        create_directory(RESULTS_PATH)\n",
        "        \n",
        "        output =  'Result' + '_' + os.path.splitext(content)[0] + '_' + os.path.splitext(model)[0]\n",
        "        output_path = Path(RESULTS_PATH + \"/\" + output + \".mp4\")\n",
        "\n",
        "        video_style_transfer(str(content_path), str(model_path), str(output_path), batch_s=4)\n",
        "\n",
        "        print(\"video saved in Results folder, refresh Files to see it\")\n",
        "        \n",
        "        files.download(output_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Select one of the following options:\n",
            "1 - Style Transfer for Images\n",
            "2 - Style Transfer for Videos\n",
            "2\n",
            "Please upload at least one video\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-595fddde-1467-4776-9290-3b0b1a15ac3a\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-595fddde-1467-4776-9290-3b0b1a15ac3a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}