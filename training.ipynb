{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Giogia/gatys_piu_bello/blob/master/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ytog81RotoZe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "from functools import reduce\n",
        "from operator import mul\n",
        "from google.colab import files\n",
        "from tensorflow.keras import Model\n",
        "from numpy import expand_dims, array\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.applications import vgg19\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from IPython.display import HTML, display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3nn1VqFWWXQj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "STYLE_LAYERS = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
        "CONTENT_LAYERS = ['block4_conv2']\n",
        "\n",
        "def net_pro(img):\n",
        "    f = get_model()\n",
        "    return f(img)\n",
        "\n",
        "def get_model():\n",
        "    model = vgg19.VGG19()\n",
        "    model.trainable = False\n",
        "    style_feature = [model.get_layer(i).output for i in STYLE_LAYERS]\n",
        "    content_feature = [model.get_layer(i).output for i in CONTENT_LAYERS]\n",
        "    return Model(model.input, style_feature + content_feature)\n",
        "  \n",
        "def get_feat_style(net, layer):\n",
        "    return net[STYLE_LAYERS.index(layer) + len(CONTENT_LAYERS)]\n",
        "  \n",
        "def get_feat_content(net, layer):\n",
        "    return net[CONTENT_LAYERS.index(layer)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PiierSM4tS_Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "WEIGHTS_INIT_STDEV = .1\n",
        "\n",
        "\n",
        "def net(image):\n",
        "\n",
        "    conv1 = conv_layer(image, 32, 9, 1)\n",
        "    conv2 = conv_layer(conv1, 64, 3, 2)\n",
        "    conv3 = conv_layer(conv2, 128, 3, 2)\n",
        "    resid1 = residual_block(conv3, 3)\n",
        "    resid2 = residual_block(resid1, 3)\n",
        "    resid3 = residual_block(resid2, 3)\n",
        "    resid4 = residual_block(resid3, 3)\n",
        "    resid5 = residual_block(resid4, 3)\n",
        "    conv_t1 = conv_tranpose_layer(resid5, 64, 3, 2)\n",
        "    conv_t2 = conv_tranpose_layer(conv_t1, 32, 3, 2)\n",
        "    conv_t3 = conv_layer(conv_t2, 3, 9, 1, is_relu=False)\n",
        "    preds = tf.nn.tanh(conv_t3) * 150 + 255./2\n",
        "\n",
        "    return preds\n",
        "\n",
        "\n",
        "def conv_layer(image, filter_number, filter_size, strides, is_relu=True):\n",
        "\n",
        "    # make the convolution of the image and return the convolution\n",
        "\n",
        "    weights_initialization = conv_initialization_vars(image, filter_number, filter_size)\n",
        "    strides_shape = [1, strides, strides, 1]\n",
        "\n",
        "    # apply the filter to the image with a 2d convolution\n",
        "    image = tf.nn.conv2d(image, weights_initialization, strides_shape, padding='SAME')\n",
        "    image = _instance_norm(image)\n",
        "\n",
        "    if is_relu:\n",
        "        image = tf.nn.relu(image)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def conv_tranpose_layer(img, filter_number, filter_size, strides):\n",
        "\n",
        "    weights_initialized = conv_initialization_vars(img, filter_number, filter_size, transpose=True)\n",
        "\n",
        "    batch_size, rows, cols, in_channels = [i for i in img.get_shape()]\n",
        "    new_rows, new_cols = int(rows * strides), int(cols * strides)\n",
        "    new_shape = [batch_size, new_rows, new_cols, filter_number]\n",
        "\n",
        "    tf_shape = tf.stack(new_shape)\n",
        "    strides_shape = [1, strides, strides, 1]\n",
        "\n",
        "    convolution = tf.nn.conv2d_transpose(img, weights_initialized, tf_shape, strides_shape, padding='SAME')\n",
        "    convolution = _instance_norm(convolution)\n",
        "\n",
        "    return tf.nn.relu(convolution)\n",
        "\n",
        "\n",
        "def residual_block(img, filter_size=3):\n",
        "\n",
        "    tmp_convolution = conv_layer(img, 128, filter_size, 1)\n",
        "\n",
        "    # add the convolution to the original image\n",
        "    return img + conv_layer(tmp_convolution, 128, filter_size, 1, is_relu=False)\n",
        "\n",
        "\n",
        "def _instance_norm(img):\n",
        "\n",
        "    # set the shape of the input img\n",
        "    batch_size, rows, cols, in_channels = [i for i in img.get_shape()]\n",
        "    var_shape = [in_channels]\n",
        "\n",
        "    # calculate the mean and the variance of the img\n",
        "    mu, sigma_sq = tf.nn.moments(img, [1, 2], keep_dims=True)\n",
        "    shift = tf.Variable(tf.zeros(var_shape))\n",
        "    scale = tf.Variable(tf.ones(var_shape))\n",
        "    epsilon = 1e-3\n",
        "\n",
        "    # normalize the img input wrt the mean and the variance calculated\n",
        "    normalized = (img - mu) / (sigma_sq + epsilon) ** (.5)\n",
        "\n",
        "    return scale * normalized + shift\n",
        "\n",
        "\n",
        "def conv_initialization_vars(network, out_channels, filter_size, transpose=False):\n",
        "    \n",
        "    _, rows, cols, in_channels = [i.value for i in network.get_shape()]\n",
        "\n",
        "    if not transpose:\n",
        "        weights_shape = [filter_size, filter_size, in_channels, out_channels]\n",
        "    else:\n",
        "        weights_shape = [filter_size, filter_size, out_channels, in_channels]\n",
        "\n",
        "    # with tf truncated we output rnd values rom a truncated normal distribution\n",
        "    weights_init = tf.Variable(tf.truncated_normal(weights_shape, stddev=WEIGHTS_INIT_STDEV, seed=1), dtype=tf.float32)\n",
        "\n",
        "    return weights_init"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fx3cccfcw8Qp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_image(path):\n",
        "\n",
        "    max_dim = 1024\n",
        "\n",
        "    img = Image.open(path)\n",
        "\n",
        "    # resize image to max_dim\n",
        "    scale = max_dim / max(img.size)\n",
        "\n",
        "    if scale < 1:\n",
        "        scaled_width = round(img.size[0] * scale)\n",
        "        scaled_height = round(img.size[1] * scale)\n",
        "        img = img.resize((scaled_width, scaled_height))\n",
        "\n",
        "    #convert greyscale to rgb\n",
        "    img = img.convert(\"RGB\")\n",
        "\n",
        "    img = img_to_array(img)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def preprocess_image(img):\n",
        "\n",
        "    img = expand_dims(img, axis=0)\n",
        "\n",
        "    #normalize by mean = [103.939, 116.779, 123.68] and with channels BGR\n",
        "    img = preprocess_input(img)\n",
        "\n",
        "    return img\n",
        "  \n",
        "\n",
        "def get_img(src, img_size=False):\n",
        "    img = scipy.misc.imread(src, mode='RGB') # misc.imresize(, (256, 256, 3))\n",
        "    if not (len(img.shape) == 3 and img.shape[2] == 3):\n",
        "        img = np.dstack((img,img,img))\n",
        "    if img_size != False:\n",
        "        img = scipy.misc.imresize(img, img_size)\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ufJLqoGMsePU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "M_PIXEL = np.array([123.68,  116.779,  103.939])\n",
        "\n",
        "\n",
        "def get_style_loss(layers, batch_size, style_features, style_weight, net):\n",
        "    style_losses = []\n",
        "    for style_layer in layers:\n",
        "        layer = get_feat_style(net, style_layer)\n",
        "        bs, height, width, filters = map(lambda i: i.value, layer.get_shape())\n",
        "        size = height * width * filters\n",
        "        feats = tf.reshape(layer, (bs, height * width, filters))\n",
        "        feats_t = tf.transpose(feats, perm=[0, 2, 1])\n",
        "        grams = tf.matmul(feats_t, feats) / size\n",
        "        style_gram = style_features[style_layer]\n",
        "        style_losses.append(2 * tf.nn.l2_loss(grams - style_gram) / style_gram.shape)\n",
        "    style_loss = style_weight * reduce(tf.add, style_losses) / batch_size\n",
        "    return style_loss\n",
        "\n",
        "\n",
        "def get_content_loss(layer, batch_size, content_features, content_weight, net):\n",
        "    lay = get_feat_content(net, layer)\n",
        "    content_size = t_size(content_features[layer]) * batch_size\n",
        "    assert t_size(content_features[layer]) == t_size(lay)\n",
        "    d_content = lay - content_features[layer]\n",
        "    content_loss = content_weight * 2 * tf.nn.l2_loss(d_content) / content_size\n",
        "    return content_loss\n",
        "\n",
        "\n",
        "def t_size(tensor):\n",
        "    return reduce(mul, (d.value for d in tensor.get_shape()[1:]), 1)\n",
        "\n",
        "\n",
        "def compute_loss(content_ph, style_target, weights, tv_weight, layers, batch_size, batch_shape, gen_network_vgg):\n",
        "    style_features = precompute_style_features(layers[1], style_target, gen_network_vgg)\n",
        "    content_features = get_content_net_and_features(layers[0], content_ph, gen_network_vgg)\n",
        "    content_ph = content_ph / 255.0\n",
        "    net = generate_net(content_ph, gen_network_vgg)\n",
        "    content_loss = get_content_loss(layers[0][0], batch_size, content_features, weights[0], net)\n",
        "    style_loss = get_style_loss(layers[1], batch_size, style_features, weights[1], net)\n",
        "\n",
        "    # total variation denoising\n",
        "    tv_y_size = t_size(content_ph[:, 1:, :, :])\n",
        "    tv_x_size = t_size(content_ph[:, :, 1:, :])\n",
        "    y_tv = tf.nn.l2_loss(content_ph[:, 1:, :, :] - content_ph[:, :batch_shape[1]-1, :, :])\n",
        "    x_tv = tf.nn.l2_loss(content_ph[:, :, 1:, :] - content_ph[:, :, :batch_shape[2]-1, :])\n",
        "    tv_loss = tv_weight*2*(x_tv/tv_x_size + y_tv/tv_y_size)/batch_size\n",
        "\n",
        "    return content_loss + style_loss + tv_loss\n",
        "\n",
        "\n",
        "def get_content_net_and_features(layers, content_ph, vgg):\n",
        "    # pre-compute content features\n",
        "    content_features = {}\n",
        "    content_net = vgg(content_ph - M_PIXEL)\n",
        "    for layer in layers:\n",
        "        content_features[layer] = get_feat_content(content_net, layer)\n",
        "    return content_features\n",
        "\n",
        "def precompute_style_features(layers, style_target, vgg):\n",
        "    style_features = {}\n",
        "    style_image = tf.placeholder(tf.float32, style_target.shape, name='style_image')\n",
        "    net = vgg(style_image - M_PIXEL)\n",
        "    style_pre = np.array(style_target)\n",
        "    for layer in layers:\n",
        "        features = get_feat_style(net, layer).eval(feed_dict={style_image: style_pre})\n",
        "        features = np.reshape(features, (-1, features.shape[3]))\n",
        "        gram = np.matmul(features.T, features) / features.size\n",
        "        style_features[layer] = gram\n",
        "    return style_features\n",
        "\n",
        "def generate_net(content_ph, vgg):\n",
        "    return vgg(net(content_ph) - M_PIXEL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o74nqUyssOsh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def optimize(content_folder, style, content_weight, style_weight, tv_weight, vgg, \n",
        "             epochs=2, batch_size=4, save_path='fns.ckpt', learning_rate=1e-3):\n",
        "    style_target = load_image(style)\n",
        "    style_target = preprocess_image(style_target)\n",
        "    content_targets = get_files(content_folder)\n",
        "    \n",
        "    batch_shape = (batch_size, 256, 256, 3)\n",
        "    layers = CONTENT_LAYERS, STYLE_LAYERS\n",
        "    weights = content_weight, style_weight\n",
        "\n",
        "    with tf.Graph().as_default(), tf.Session() as sess:\n",
        "        x_content = tf.placeholder(tf.float32, shape=batch_shape, name=\"x_content\")\n",
        "        # overall loss\n",
        "        loss = compute_loss(x_content, style_target, weights, tv_weight, layers, batch_size, batch_shape, vgg)\n",
        "        train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        progressbar = display(progress(0,epochs), display_id=True)\n",
        "        for epoch in range(epochs):\n",
        "            sample = len(content_targets)\n",
        "            it = 0\n",
        "            while it * batch_size < sample:\n",
        "                curr = it * batch_size\n",
        "                step = curr + batch_size\n",
        "                x_batch = [get_img(img_p, (256, 256, 3)).astype(np.float32) for img_p in content_targets[curr:step]]\n",
        "                x_batch = np.array(x_batch, dtype=np.float32)\n",
        "                train_step.run(feed_dict={x_content: x_batch})\n",
        "                it += 1\n",
        "            progressbar.update(progress(it%epochs, epochs))\n",
        "        saver = tf.train.Saver()\n",
        "        saver.save(sess, save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rRK-kWPIk127",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "roSx8i_fuGZR",
        "colab_type": "code",
        "outputId": "177dea74-8a26-4886-bf7d-0525b8b21dd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1696
        }
      },
      "cell_type": "code",
      "source": [
        "PATH = os.getcwd()\n",
        "IMAGES_PATH = PATH + '/Images'\n",
        "CONTENT_IMAGE_PATH = IMAGES_PATH + '/Content'\n",
        "\n",
        "\n",
        "def find_file(filename, directory):\n",
        "    for file in os.listdir(directory):\n",
        "        if os.path.splitext(file)[0] == filename:\n",
        "            return file\n",
        "\n",
        "    return None\n",
        "\n",
        "    \n",
        "def create_directory(path):\n",
        "    \n",
        "    if not(os.path.isdir(path)):\n",
        "            os.mkdir(path)\n",
        "  \n",
        "  \n",
        "def upload_files(path,number, message):\n",
        "    \n",
        "    while(len(os.listdir(path))<number):\n",
        "            print(\"Please upload at least \"+message)\n",
        "            os.chdir(path)\n",
        "            files.upload()\n",
        "            os.chdir(PATH)\n",
        "            time.sleep(30)\n",
        "    \n",
        "      \n",
        "def list_files(path):\n",
        "  \n",
        "    for file in os.listdir(Path(path)):\n",
        "            print(os.path.splitext(file)[0])\n",
        "        \n",
        "def get_files(path):\n",
        "    return [os.path.join(path,os.path.splitext(file)[0] + \n",
        "                         os.path.splitext(file)[1]) for file in os.listdir(Path(path))]\n",
        "\n",
        "def check_error(var,path):\n",
        "  \n",
        "    while(var==None):\n",
        "            print(\"Please insert a correct Name (case sensitive input)\")\n",
        "            list_files(path)\n",
        "            var = find_file(input(),Path(path))\n",
        "            \n",
        "    return var\n",
        "  \n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    create_directory(IMAGES_PATH)\n",
        "    create_directory(CONTENT_IMAGE_PATH)\n",
        "    \n",
        "\n",
        "    print(\"Upload Content Images:\")\n",
        "    \n",
        "    upload_files(CONTENT_IMAGE_PATH,10,\" at least 10000 images\")\n",
        "    \n",
        "    print(\"Using the following images for content training:\")\n",
        "    list_files(CONTENT_IMAGE_PATH)\n",
        "    \n",
        "    content_path  = Path(CONTENT_IMAGE_PATH + '/')\n",
        "\n",
        "    print(\"Select Style Image:\")\n",
        "    \n",
        "    upload_files(IMAGES_PATH,2,\"style\")\n",
        "    \n",
        "    list_files(IMAGES_PATH)\n",
        "\n",
        "    style = find_file(input(),Path(IMAGES_PATH))\n",
        "    style = check_error(style,IMAGES_PATH)\n",
        "    style_path = Path(IMAGES_PATH + \"/\" + style)\n",
        "\n",
        "    output =  'Model_' + os.path.splitext(style)[0]\n",
        "    output_path = Path(PATH +\"/\"+ output + \".ckpt\")\n",
        "    open(output + \".ckpt\", \"wb\").close\n",
        "    content_weight = 7.5e0\n",
        "    style_weight = 1e2\n",
        "    tv_weight = 2e2\n",
        "    batch_size = 4\n",
        "    epochs = 2\n",
        "    learning_rate=1e-3\n",
        "    optimize(content_path, style_path, content_weight, style_weight, tv_weight, \n",
        "             net_pro, epochs, batch_size, output_path, learning_rate)\n",
        "    \n",
        "    print(\"train model saved in Files folder, refresh Files to see it\")"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Upload Content Images:\n",
            "Using the following images for content training:\n",
            "Chicago\n",
            "Art\n",
            "Atlanta\n",
            "AirBalloons\n",
            "Hawaii\n",
            "Abstract\n",
            "Country\n",
            "Deer\n",
            "Barca\n",
            "Munch\n",
            "Desert\n",
            "Flowers\n",
            "Select Style Image:\n",
            "Wave\n",
            "Content\n",
            "Wave\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        <progress\n",
              "            value='1'\n",
              "            max='2',\n",
              "            style='width: 100%'\n",
              "        >\n",
              "            1\n",
              "        </progress>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "InternalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInternalError\u001b[0m: Unable to get element as bytes.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-218-d3e4bbcc09e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     optimize(content_path, style_path, content_weight, style_weight, tv_weight, \n\u001b[0;32m---> 83\u001b[0;31m              net_pro, epochs, batch_size, output_path, learning_rate)\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train model saved in Files folder, refresh Files to see it\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-216-c2a0dc97ce42>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(content_folder, style, content_weight, style_weight, tv_weight, vgg, epochs, batch_size, save_path, learning_rate)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mprogressbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1439\u001b[0m           model_checkpoint_path = sess.run(\n\u001b[1;32m   1440\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInternalError\u001b[0m: Unable to get element as bytes."
          ]
        }
      ]
    }
  ]
}