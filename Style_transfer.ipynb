{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Style_transfer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Giogia/gatys_piu_bello/blob/master/Style_transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "xVS3ikr1mehX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "IMPORT"
      ]
    },
    {
      "metadata": {
        "id": "URLc5gsjl_Jt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.eager as tfe\n",
        "import tensorflow.keras as models\n",
        "\n",
        "from google.colab import files\n",
        "from tensorflow import clip_by_value\n",
        "from tensorflow import enable_eager_execution\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from numpy import clip, expand_dims, squeeze, array, random\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BWb96dmdgows",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "CNN"
      ]
    },
    {
      "metadata": {
        "id": "sVwQ_6NhgSRd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VGG19_c:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        # here you say where you want to take the features for the content\n",
        "        self.contentLayers = ['block4_conv2']\n",
        "\n",
        "        # here you say where you want to take the features for the style\n",
        "        self.styleLayers = ['block1_conv1',\n",
        "                              'block2_conv1',\n",
        "                              'block3_conv1',\n",
        "                              'block4_conv1',\n",
        "                              'block5_conv1']\n",
        "        self.content_layers_num = len(self.contentLayers)\n",
        "        self.style_layers_num = len(self.styleLayers)\n",
        "\n",
        "\n",
        "        self.model = self.getModel()\n",
        "\n",
        "        # after setting model not trainable we also set the layers not trainable\n",
        "        for layer in self.model.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "\n",
        "    def get_content_features(self,img):\n",
        "        return self.get_output_features(img)[0]\n",
        "\n",
        "\n",
        "    def get_style_features(self, img):\n",
        "        return self.get_output_features(img)[1]\n",
        "\n",
        "\n",
        "    def get_output_features(self, content):\n",
        "\n",
        "        features = self.model(content)\n",
        "\n",
        "        # for the content take only the content layers from 0 to len of content\n",
        "        content = [style_content[0] for style_content in features[self.style_layers_num:]]\n",
        "\n",
        "        # for style take only the style layers from len of content to len of content + len of style\n",
        "        style = [style[0] for style in features[:self.style_layers_num]]\n",
        "\n",
        "        return content, style\n",
        "\n",
        "\n",
        "    def getModel(self):\n",
        "\n",
        "        # we load the VGG19 pretrained with the dataset imagenet and we don't include the 3 fully connected layers on\n",
        "        # top of theVGG19\n",
        "        vgg = VGG19(include_top=False, weights='imagenet')\n",
        "\n",
        "        # we freeze the weights and the variables\n",
        "        vgg.trainable = False\n",
        "\n",
        "        style_feature = []\n",
        "        for i in self.styleLayers:\n",
        "            style_feature.append(vgg.get_layer(i).output)\n",
        "\n",
        "        content_feature = []\n",
        "        for i in self.contentLayers:\n",
        "            content_feature.append(vgg.get_layer(i).output)\n",
        "\n",
        "        #using the Keras API we return the model of the CNN\n",
        "        return models.Model(vgg.input, style_feature + content_feature)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bYTUfT2JgtV_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "IMAGE"
      ]
    },
    {
      "metadata": {
        "id": "p32FYPgpgdoO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_image(path):\n",
        "\n",
        "    max_dim = 512\n",
        "\n",
        "    img = Image.open(path)\n",
        "\n",
        "    # resize image to max_dim\n",
        "    scale = max_dim / max(img.size)\n",
        "    scaled_width = round(img.size[0] * scale)\n",
        "    scaled_height = round(img.size[1] * scale)\n",
        "    img = img.resize((scaled_width, scaled_height))\n",
        "\n",
        "    img = img_to_array(img)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def save_image(path, img):\n",
        "\n",
        "    img = Image.fromarray(clip(img, 0, 255).astype('uint8'))\n",
        "    img.save(path, 'JPEG')\n",
        "\n",
        "\n",
        "def preprocess_image(img):\n",
        "\n",
        "    img = expand_dims(img, axis=0)\n",
        "\n",
        "    #normalize by mean = [103.939, 116.779, 123.68] and with channels BGR\n",
        "    img = preprocess_input(img)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def postprocess_image(processed_img):\n",
        "\n",
        "    img = processed_img.copy()\n",
        "\n",
        "    # shape (1, h, w, d) to (h, w, d)\n",
        "    if len(img.shape) == 4:\n",
        "        img = squeeze(img, axis=0)\n",
        "    if len(img.shape) != 3:\n",
        "        raise ValueError(\"Invalid input to deprocessing image\")\n",
        "\n",
        "    # Remove VGG mean\n",
        "    img[:, :, 0] += 103.939\n",
        "    img[:, :, 1] += 116.779\n",
        "    img[:, :, 2] += 123.68\n",
        "\n",
        "    # rgb to bgr\n",
        "    img = img[:, :, ::-1]\n",
        "\n",
        "    #cast to values within (-255,255)\n",
        "    img = clip(img, 0, 255).astype('uint8')\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def clip_image(img):\n",
        "\n",
        "    norm_means = array([103.939, 116.779, 123.68])\n",
        "    min_vals = -norm_means\n",
        "    max_vals = 255 - norm_means\n",
        "\n",
        "    img = clip_by_value(img, min_vals, max_vals)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def show_image(img, title=None):\n",
        "\n",
        "    # Normalize for display\n",
        "    out = img.astype('uint8')\n",
        "\n",
        "    # Remove the batch dimension\n",
        "    if len(img.shape) == 4:\n",
        "        out = squeeze(out, axis=0)\n",
        "\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "\n",
        "    plt.imshow(out)\n",
        "\n",
        "\n",
        "def generate_noise_image(img):\n",
        "\n",
        "    img = random.uniform(-20,20,img.shape).astype('uint8')\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def show_content_style(content_path, style_path):\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "\n",
        "    content = load_image(content_path)\n",
        "    style = load_image(style_path)\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    show_image(content, 'Content')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    show_image(style, 'Style')\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cSkZidwqg4Uy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "LOSS"
      ]
    },
    {
      "metadata": {
        "id": "jKEPV9Mfg07z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def g_matrix(tensor):\n",
        "\n",
        "    channels = int(tensor.shape[-1])\n",
        "\n",
        "    # reshape as 1-Dim array dividing it per channel\n",
        "    a = tf.reshape(tensor, [-1, channels])\n",
        "\n",
        "    # compute the matrix a*a^t and then divide by the dimension\n",
        "    return tf.matmul(a, a, transpose_a=True) / tf.cast(tf.shape(a)[0], tf.float32)\n",
        "\n",
        "\n",
        "def get_content_loss(content, target):\n",
        "\n",
        "    return tf.reduce_mean(0.5*tf.square(content - target))\n",
        "\n",
        "\n",
        "def get_style_loss(style, g_target):\n",
        "\n",
        "    g_style = g_matrix(style)\n",
        "    height, width, channels = style.get_shape().as_list()\n",
        "    weight = (channels ** 2)\n",
        "\n",
        "\n",
        "    return tf.reduce_mean(tf.square(g_style - g_target))/weight\n",
        "\n",
        "\n",
        "def accumulate_loss(img_feature, layers_n, noise_feature, loss):\n",
        "\n",
        "    score = 0\n",
        "    weight_per_layer = 1.0 / float(layers_n)\n",
        "\n",
        "    for target, comb_content in zip(img_feature, noise_feature):\n",
        "        score += weight_per_layer * loss(comb_content, target)\n",
        "\n",
        "    return score\n",
        "\n",
        "\n",
        "def compute_loss(noise_features, img_features, loss_w, layers_n):\n",
        "    \"\"\"This function will compute the loss total loss.\n",
        "\n",
        "    Arguments:\n",
        "      noise_features: The content and style of the white noise\n",
        "      loss_w: The weights of each contribution of each loss function.\n",
        "        (style weight, content weight, and total variation weight)\n",
        "      img_features: Content and style features\n",
        "      loss_w: Weights of the elements\n",
        "      layers_n: Number of content and style layers\n",
        "\n",
        "    Returns:\n",
        "      returns the total loss\n",
        "    \"\"\"\n",
        "\n",
        "    # Accumulate content losses from all layers\n",
        "    content_score = accumulate_loss(img_features[0], layers_n[0], noise_features[0], get_content_loss)\n",
        "\n",
        "    # Accumulate style losses from all layers\n",
        "    style_score = accumulate_loss(img_features[1], layers_n[1], noise_features[1], get_style_loss)\n",
        "\n",
        "    # Here, we equally weight each contribution of each loss layer\n",
        "    content_score *= loss_w[0]\n",
        "    style_score *= loss_w[1]\n",
        "\n",
        "    return style_score + content_score\n",
        "\n",
        "\n",
        "def compute_gradient(noise_img, noise_features_gen, img_features, loss_w, layers_n):\n",
        "\n",
        "    with tf.GradientTape() as g:\n",
        "        loss = compute_loss(noise_features_gen(noise_img), img_features, loss_w, layers_n)\n",
        "\n",
        "    # Compute gradients wrt input image\n",
        "    return g.gradient(loss, noise_img), loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IQwxfvI-gLkj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_style_transfer(content_path, style_path, iterations=1000, content_weight=1e0, style_weight=1e1, learning_rate=5):\n",
        "\n",
        "    #create images\n",
        "    content = load_image(content_path)\n",
        "    style = load_image(style_path)\n",
        "    noise = generate_noise_image(content)\n",
        "\n",
        "    content = preprocess_image(content)\n",
        "    style = preprocess_image(style)\n",
        "    noise = preprocess_image(noise)\n",
        "    percentage = 0.05\n",
        "    noise = percentage*noise+(1-percentage)*content\n",
        "\n",
        "    noise = tfe.Variable(noise, dtype=tf.float32)\n",
        "\n",
        "    # create model\n",
        "    vgg = VGG19_c()\n",
        "    loss_weights = content_weight, style_weight\n",
        "    layers_number = vgg.content_layers_num , vgg.style_layers_num\n",
        "\n",
        "    #create features\n",
        "    content_features = vgg.get_content_features(content)\n",
        "    style_features = vgg.get_style_features(style)\n",
        "    gram_matrix_features = [g_matrix(feature) for feature in style_features]\n",
        "\n",
        "    img_features = content_features, gram_matrix_features\n",
        "\n",
        "    #create optimizer\n",
        "    opt = tf.train.AdamOptimizer(learning_rate, beta1=0.99, epsilon=1e-1)\n",
        "\n",
        "    #store best results\n",
        "    best_loss, best_img = float('inf'), None\n",
        "\n",
        "    plt.ion()\n",
        "    for i in range(iterations):\n",
        "\n",
        "        grads, loss = compute_gradient(noise,vgg.get_output_features,img_features,loss_weights,layers_number)\n",
        "\n",
        "        opt.apply_gradients([(grads, noise)])\n",
        "\n",
        "        clipped = clip_image(noise)\n",
        "        noise.assign(clipped)\n",
        "\n",
        "\n",
        "        if loss < best_loss:\n",
        "\n",
        "            # Update best loss and best image from total loss.\n",
        "            best_loss = loss\n",
        "            best_img = postprocess_image(noise.numpy())\n",
        "        if i %100 == 0:\n",
        "            print(i)\n",
        "            print(loss,\"best\",best_loss)\n",
        "            plot_img = noise.numpy()\n",
        "            plot_img = postprocess_image(plot_img)\n",
        "            show_image(plot_img)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    save_image('Result.jpg',best_img)\n",
        "\n",
        "    return best_loss, best_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K2vd75pWgxOB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "STYLE TRANSFER"
      ]
    },
    {
      "metadata": {
        "id": "bRiZZYOFiJmM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.enable_eager_execution()\n",
        "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
        "\n",
        "content_path = 'Chicago.jpg'\n",
        "style_path = 'Atlanta.jpg'\n",
        "\n",
        "show_content_style(content_path, style_path)\n",
        "\n",
        "best, best_loss = run_style_transfer(content_path, style_path, iterations=2000)\n",
        "\n",
        "files.download('Result.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}